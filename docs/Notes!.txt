Довольно часто сеть попадает в локальный минимум
Иногда всё хорошо срабатывает


К случайному блужданию
    * Если добавление был удачным, то разумно попробовать его повторить. 

Еще не забыть эволюционный поиск минимума

Вероятно, той ситуации, когда происходит застревание и при этом все выходные значения для
каждого из входных образцов совпадают, - соотвествуют почти нулевые веса всех контактов,
и ненулевой вес bias - то есть сдвига последнего нейрона.


Пока не полностью осознанное: использование кросс-энтропии для функции ошибки
Регуляризация L2 и L1 - по всей видимости уменьшает застревание в некоторых локальных минимумах

* Еще я вспомнил, что забыл сделать сдвиг - bias - для внутренних нейронов

* А что если сделать сдвиг не только линейный - для каждого нейрона - поиск градиента тогда конечно осложнится немного



Mon 12/19/2016 
Не забыть что на время отладки функция getMeanSquareError() возвращает fakeCostFunction

Несколько дней назад сделал так чтобы все генераторы случайных чисел были детерминированы
В WeightedRandom при инициализации передаётся объект Random, и если он не инициализирован - то он рандомизируется таймером
Поэтому я везде избавился от Math.random()

Итак, градиентный поиск работает
Временами работает хуже чем при случайном блуждании. 

* Удостовериться, что уменьшим шаг случайного блуждания мы получаем новые застревания

* При меньшении шага случайного блуждания - алгоритм в сущности вырождается в градиентный спуск - интересно удостовериться.


12/19/2016 1:51pm
Для упрощения и сведения градиента ошибки всех образцов - к градиенту ошибки на одном образце возможно имеет смысл
рассматривать тренировочное множество из ОДНОГО элемента.

12/24/2016
  На данный момент в некотором роде есть д у б л и р о в а н и е - в части программы,
  которая отвечает за обучение случайным блужданием в т.ч. в getMeanSquareError()
   
  
  
  Эксперименты с коэффициентами:
  скрытый слой: 1, 10, 10, 1 - то есть 22 скрытых нейрона

  Медленный градиентный поиск успевает сделать 10 итераций за 1180 миллисекунд
  А вот градиентный поиск с обратным распространением уже занимает 17 миллисекунд. Т.е. в ~70 раз быстрее


            network.LEARNING_RATE = 10 и при = 1;
            network.DELTA_W = 0.1;
            Backpropagation застревает

 

            network.LEARNING_RATE = 1;
            network.DELTA_W = 0.01;
            А вот теперь (после уменьшения delta_W ) очень быстро находит минимум (немного континтуитивно - то что уменьшение этого параметра только ускоряет поиск минимума)


            network.LEARNING_RATE = 10;
            network.DELTA_W = 0.01;
            А тут уже всё скорее похоже на застревание



            А вот со случайным блужданием получается очень интересно!
            Если установить network.randomWeightAdditionRange = 0.05, то обучение довольно быстро очень замедляется и почти останавливается

            А вот при уменьшении до network.randomWeightAdditionRange = 0.01, ошибка начинает спадать ощутимо: при маленькой случайной добавке случайное блуждание с отбором переходит в градиентные спуск


Добавляю нормализацию ошибки в backpropagation

Мне кажется стоит убрать деление ошибки на кол-во образцов в мини-пакете

Что делать с DELTA_W? С одной стороны для прямолинейного градиентного спуска оно необходимо
С другой - backpropagation в сущности вычисляет аналитическую производную функции



при переносе деления на DELTA_W кажется

12201 миллисекунд
11596 миллисекунд
11948

700 миллисекунд выигрыша на 1000 циклов - в методе slowGradientDescent
10 тысяч циклов - 7 секунд выигрыша
100 тысяч - 70 секунд выигрыша

1185



Пока не буду оптимизировать slowGradientDescent - он необходим в первую очередь для того, чтобы удостоверяться что правильно работает быстрый градиентный спуск calculateBackpropagationGradient.

На данный момент если выводить и сравнивать результаты просчетов градиента после вызова slowGradientDescent() и teachWithBackpropagation() - они отличаются множителем - на количество тренировочных образцов - потому что в случае teachWithBackpropagation я делю градиент на их число уже при вычитании градента от массива весов.

Лог исправлений

Исправления в getMeanSquareError():
    Исправил проход по образцам в getMeanSquareError() - теперь цикл пробегается по мини-пакету.

    Вместо деления на inputs.length использую граници мини-пакета:
    sumError = sumError / (miniBatchEndIndex - miniBatchStartIndex + 1);

Исправление в teachWithBackpropagation():
    FromArraySubtractArrayTimesAlpha(weight, costGradient,  LEARNING_RATE  / (miniBatchEndIndex - miniBatchStartIndex + 1));


    Не забыть сравнить градиента с ограниченным мини-пакетом!!!

Как можно было бы предварительно изучить характер многомерной поверхности образцов текущего набора данных?


26 января 2017 раннее утро 6:30

Даже объем тренировочных данных с цифрами mnist - если каждую из точек переводить в тип double64 - будет занимать (60 000 + 10 000) * 28 * 28 * 8 байтов ~ 419Мб

Возможно, имеет смысл по крайней мере все нейроны входного слоя сделать байтовыми с нормализацией
Может быть даже веса и заряды можно перевести в byte или double byte (word).

Интересно, но:
В Яве массив double[70000*28*28] занимает приблизительно 431 Мб - именно на такую величину увеличивается занимаемая программой память при отображении в ProcessExplorer после инициализиции массива. Оценка даёт (70000*28*28*8/1024/1024 = 418Мб). Т.е всё очень резонно и похоже на правду.

___________________________________________________________________________
import java.util.Scanner;
public class SignedVSUnsigned1 {
    public static void main(String[] args) {

        Scanner in = new Scanner(System.in);
        System.out.println("Press ENTER to continue"); // Смотрим на выделенную память в ProcessExplorer тут
        in.nextLine();

        double[] arr = new double[70000*28*28];
        System.out.println("Press ENTER to exit");
        in.nextLine();                                  // И сравниваем выделенную программе память тут с тем что сообщал ProcessExplorer до этого
        arr[0] = 23;
    }
}
___________________________________________________________________________

А вот после того, как я превращаю одномерный массив в двумерный, - всё в корне меняется:
        .......
        double[][] arr = new double[70000][28*28];
        System.out.println("Press ENTER to exit");
        in.nextLine();
        arr[0][0] = 23;
        .......
По подсчетам ProcessExplorer'а при определении массива выделилось 578 Мб. 
Т.е. на 160 Мб больше, чем было в случае одномерного массива.
Хотя для 70 тысяч указателей этого многовато. 70 000 * 8 байт ~ 0.5 Мб
Дело, возможно, в каких-то минимальных порциях памяти, выделяемых ОС.


В действительности я забыл совсем - что каждый из 70 тыс массивов - объект, у которого есть поля, методы по умолчанию и т.д. В случае с массивами выходит что эти "лишние" 160 мегабайтов тратятся на накладные расходы на содержание объекта, и полей которые я сам использую - например length.







Итак, довольно интересно выходит, что добавление случайного блуждания к процедуре обучения 
немного как бы обезопашивает от застреваний

Тем не менее - довольно часто встречается перестрел - который довольно просто засекается
Его можно пытаться засекать не на каждой отдельной итерации, а например через 100 или 1000
итераций обратного распространения.

Иногда - и это заметно - модуль градиента становится таким маленьким, что имеет смысл
в процессе обучения увеличивать LEARNING RATE






Интересно
Инициализация весов нулями показывает себя гораздо лучше чем инициализация случайными числами
Нужно пожалуй сделать еще нормализацию входных сигналов на единицу

Можно будет подправить сеть, чтобы она принимала на входы массив вида byte.
Это позволит экономить место в памяти и использовать гораздо бОльшие дата-сеты.




Может гораздо оптимальнее сделать десять сетей для определения каждой цифры и дать им голосовать?
08.02.17




28 июля 2017
будут ли эти скачки функции ошибки при увеличении lr, если
размер пакета будет покрывать весь дата-сет


Опять глаз удивляет видимо парадоксальная ситуация:
когда точность (accuracy) уменьшается, при том что ошибка тоже уменьшается. Противоречия никакого впрочем нет

