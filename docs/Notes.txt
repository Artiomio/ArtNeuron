Довольно часто сеть попадает в локальный минимум
Иногда всё хорошо срабатывает


К случайному блужданию
    * Если добавление был удачным, то разумно попробовать его повторить. 

Еще не забыть эволюционный поиск минимума

Вероятно, той ситуации, когда происходит застревание и при этом все выходные значения для
каждого из входных образцов совпадают, - соотвествуют почти нулевые веса всех контактов,
и ненулевой вес bias - то есть сдвига последнего нейрона.


Пока не полностью осознанное: использование кросс-энтропии для функции ошибки
Регуляризация L2 и L1 - по всей видимости уменьшает застревание в некоторых локальных минимумах

* Еще я вспомнил, что забыл сделать сдвиг - bias - для внутренних нейронов

* А что если сделать сдвиг не только линейный - для каждого нейрона - поиск градиента тогда конечно осложнится немного



Mon 12/19/2016 
Не забыть что на время отладки функция getMeanSquareError() возвращает fakeCostFunction

Несколько дней назад сделал так чтобы все генераторы случайных чисел были детерминированы
В WeightedRandom при инициализации передаётся объект Random, и если он не инициализирован - то он рандомизируется таймером
Поэтому я везде избавился от Math.random()

Итак, градиентный поиск работает
Временами работает хуже чем при случайном блуждании. 

* Удостовериться, что уменьшим шаг случайного блуждания мы получаем новые застревания

* При меньшении шага случайного блуждания - алгоритм в сущности вырождается в градиентный спуск - интересно удостовериться.


12/19/2016 1:51pm
Для упрощения и сведения градиента ошибки всех образцов - к градиенту ошибки на одном образце возможно имеет смысл
рассматривать тренировочное множество из ОДНОГО элемента.

12/24/2016
  На данный момент в некотором роде есть д у б л и р о в а н и е - в части программы,
  которая отвечает за обучение случайным блужданием в т.ч. в getMeanSquareError()
   
  
  
  Эксперименты с коэффициентами:
  скрытый слой: 1, 10, 10, 1 - то есть 22 скрытых нейрона

  Медленный градиентный поиск успевает сделать 10 итераций за 1180 миллисекунд
  А вот градиентный поиск с обратным распространением уже занимает 17 миллисекунд. Т.е. в ~70 раз быстрее


            network.LEARNING_RATE = 10 и при = 1;
            network.DELTA_W = 0.1;
            Backpropagation застревает

 

            network.LEARNING_RATE = 1;
            network.DELTA_W = 0.01;
            А вот теперь (после уменьшения delta_W ) очень быстро находит минимум (немного континтуитивно - то что уменьшение этого параметра только ускоряет поиск минимума)


			network.LEARNING_RATE = 10;
            network.DELTA_W = 0.01;
            А тут уже всё скорее похоже на застревание



            А вот со случайным блужданием получается очень интересно!
            Если установить network.randomWeightAdditionRange = 0.05, то обучение довольно быстро очень замедляется и почти останавливается

            А вот при уменьшении до network.randomWeightAdditionRange = 0.01, ошибка начинает спадать ощутимо: при маленькой случайной добавке случайное блуждание с отбором переходит в градиентные спуск


Добавляю нормализацию ошибки в backpropagation

Мне кажется стоит убрать деление ошибки на кол-во образцов в мини-пакете

Что делать с DELTA_W? С одной стороны для прямолинейного градиентного спуска оно необходимо
С другой - backpropagation в сущности вычисляет аналитическую производную функции



при переносе деления на DELTA_W кажется

12201 миллисекунд
11596 миллисекунд
11948

700 миллисекунд выигрыша на 1000 циклов - в методе slowGradientDescent
10 тысяч циклов - 7 секунд выигрыша
100 тысяч - 70 секунд выигрыша

1185



Пока не буду оптимизировать slowGradientDescent - он необходим в первую очередь для того, чтобы удостоверяться что правильно работает быстрый градиентный спуск calculateBackpropagationGradient.

На данный момент если выводить и сравнивать результаты просчетов градиента после вызова slowGradientDescent() и teachWithBackpropagation() - они отличаются множителем - на количество тренировочных образцов - потому что в случае teachWithBackpropagation я делю градиент на их число уже при вычитании градента от массива весов.

Лог исправлений

Исправления в getMeanSquareError():
	Исправил проход по образцам в getMeanSquareError() - теперь цикл пробегается по мини-пакету.

	Вместо деления на inputs.length использую граници мини-пакета:
	sumError = sumError / (miniBatchEndIndex - miniBatchStartIndex + 1);

Исправление в teachWithBackpropagation():
	FromArraySubtractArrayTimesAlpha(weight, costGradient,  LEARNING_RATE  / (miniBatchEndIndex - miniBatchStartIndex + 1));


	Не забыть сравнить градиента с ограниченным мини-пакетом!!!

Как можно было бы предварительно изучить характер многомерной поверхности образцов текущего набора данных?









